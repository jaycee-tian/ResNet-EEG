{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only use one GPU\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import setpath\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.datasets import VisionDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from run.start import get_device\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from utils.eegutils import get_simple_log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gaussian_noise_images(num_images, mean, std, ran_mean, ran_std , label):\n",
    "    images = []\n",
    "    for _ in range(num_images):\n",
    "        image = np.random.normal(mean, std, (1, 224, 224))\n",
    "        if ran_mean != 0 and ran_std != 0:\n",
    "            add_image = np.random.normal(ran_mean, ran_std, (1, 224, 224))\n",
    "            image = image + add_image\n",
    "        images.append((image, label))\n",
    "    return images\n",
    "\n",
    "class GaussianNoiseDataset(VisionDataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        super().__init__('', transform=transform)\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, label = self.data[index]\n",
    "        image = torch.from_numpy(image).float()\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 40\n",
    "# 500 samples for each class, 40 classes\n",
    "num_train_samples = 50\n",
    "# 200 samples for each class, 40 classes\n",
    "num_test_samples = 100\n",
    "mean = 0\n",
    "std = 1\n",
    "\n",
    "train_data = generate_gaussian_noise_images(\n",
    "    num_train_samples, mean, std,0,0,label=0)\n",
    "test_data = generate_gaussian_noise_images(\n",
    "    num_test_samples, mean, std,0,0, label=0)\n",
    "\n",
    "ran_std_list = []\n",
    "\n",
    "ran_mean = np.random.uniform(0, 1)\n",
    "ran_std = np.random.uniform(0.5, 1.5)\n",
    "\n",
    "ran_mean_a = np.random.uniform(-1, 1)\n",
    "ran_std_a = np.random.uniform(0.5, 1.5)\n",
    "\n",
    "for i in range(num_classes-1):\n",
    "    # ran_mean_a = np.random.uniform(0, 1)\n",
    "    # ran_mean = mean\n",
    "    # ran_std_list.append(ran_std)\n",
    "# ....\n",
    "    # ran_mean_b = 0\n",
    "    # ran_mean_a = np.random.uniform(0, 1)\n",
    "    # ran_mean_b = np.random.uniform(0.1, 1)\n",
    "    # ran_std = np.random.uniform(0.5, 1.5)\n",
    "    # ran_std = np.random.uniform(0.5, 1.5)\n",
    "    train_data += generate_gaussian_noise_images(\n",
    "        num_train_samples, mean, std, 0, 0, i+1)\n",
    "    test_data += generate_gaussian_noise_images(\n",
    "        num_test_samples, mean, std, 0 , 0, i+1)\n",
    "print(ran_std_list)\n",
    "train_dataset = GaussianNoiseDataset(train_data, None)\n",
    "test_dataset = GaussianNoiseDataset(test_data, None)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run.resnet import TesNet\n",
    "\n",
    "log_dir = get_simple_log_dir()\n",
    "summary = SummaryWriter(log_dir=log_dir)\n",
    "device = 'cuda:0'\n",
    "model = TesNet(num_classes=num_classes,pretrained=False).to(device)\n",
    "# freeze the embedding layer\n",
    "# for param in model.fea_e.parameters():\n",
    "#     param.requires_grad = False\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1e6\n",
    "epoch = 0\n",
    "best_train_loss = 1000\n",
    "best_train_acc = 0\n",
    "patience = 10\n",
    "while True:\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        _,_,outputs = model(inputs.repeat(1, 3, 1, 1))\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        correct += (torch.max(outputs, 1)[1] == labels).sum().item()\n",
    "        running_loss += loss.item()\n",
    "        summary.add_scalar('train_loss', loss.item(), epoch * len(train_loader) + i)\n",
    "        epoch += 1\n",
    "    train_acc = correct / len(train_loader.dataset)*100\n",
    "    summary.add_scalar('train_acc', train_acc, epoch)\n",
    "    # use early stopping to stop training\n",
    "    # if train_acc > best_train_acc:\n",
    "    #     best_train_acc = train_acc\n",
    "    #     patience = 10\n",
    "    #     print('patience: ', patience, 'epoch: ', epoch)\n",
    "    # else:\n",
    "    #     patience -= 1\n",
    "    #     print('patience: ', patience, 'epoch: ', epoch)\n",
    "    #     if patience == 0:\n",
    "    #         break\n",
    "    \n",
    "    if epoch > num_epochs:\n",
    "        break\n",
    "    \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            _,_,outputs = model(images.repeat(1, 3, 1, 1))\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    # print test acc %, test loss\n",
    "    # print(f'Epoch {epoch + 1}, Test Accuracy: %.3f %%' % (100 * correct / total))\n",
    "    summary.add_scalar('test_acc', 100 * correct / total, epoch)\n",
    "    summary.add_scalar('test_loss', test_loss / len(test_loader), epoch)\n",
    "\n",
    "print('Finished Training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tian",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
