{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "# a=1e-40*3.27332/math.log(2)\n",
    "a=math.sqrt(2)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=0.2e-100\n",
    "print(a/b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.zeros_like([2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clip_img(img):\n",
    "    denominator = img.max() - img.min()\n",
    "    if denominator == 0:\n",
    "        # handle the case where the denominator is zero\n",
    "        normalized_img = np.zeros_like(img)\n",
    "    else:\n",
    "        normalized_img = np.nan_to_num((img - img.min()) / denominator)\n",
    "    # if is_bad_img(normalized_img):\n",
    "    #     print('bad img')\n",
    "    return normalized_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.random.randn(10,10)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(img),np.min(img),np.median(img),np.mean(img),np.std(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "from utils.start import get_gpu_usage\n",
    "\n",
    "\n",
    "def get_device(mode='auto',gpu=0):\n",
    "    if mode == 'auto':\n",
    "        return torch.device(f\"cuda:{get_gpu_usage()}\" if torch.cuda.is_available() else \"cpu\")\n",
    "    return torch.device('cuda:'+str(gpu) if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 40, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_freest_gpu():\n",
    "    \"\"\"Returns the GPU device with the most available memory.\"\"\"\n",
    "    devices = [torch.device('cuda:{}'.format(i)) for i in range(torch.cuda.device_count())]\n",
    "    print(devices[0])\n",
    "    free_mem = [torch.cuda.memory_allocated(device) for device in devices]\n",
    "    print(free_mem)\n",
    "    return devices[free_mem.index(max(free_mem))]\n",
    "\n",
    "device = get_freest_gpu()\n",
    "print(torch.cuda.device_count())\n",
    "print('Using device:', device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "torch.cuda.memory_allocated(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def get_gpu_usage():\n",
    "    \"\"\"Returns a list of dictionaries containing GPU usage information.\"\"\"\n",
    "    output = subprocess.check_output(['nvidia-smi', '--query-gpu=memory.used,memory.total', '--format=csv,nounits,noheader'], encoding='utf-8')\n",
    "    lines = output.strip().split('\\n')\n",
    "    # print(lines)\n",
    "    # gpu memory left percentage\n",
    "    gpu_info = []\n",
    "    for line in lines:\n",
    "        memory_used, memory_total = line.strip().split(',')\n",
    "        gpu_info.append(int(int(memory_used) / int(memory_total)*100))\n",
    "    print(gpu_info)\n",
    "    # return least use gpu\n",
    "    return gpu_info.index(min(gpu_info))\n",
    "\n",
    "print(get_gpu_usage())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mresnet\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mResNet\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model \u001b[39m=\u001b[39m ResNet(num_classes\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "\n",
    "import models.resnet as ResNet\n",
    "model = ResNet(num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tian",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
